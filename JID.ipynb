{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lclhome/dli/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from graphs import FFN_dae_lh_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load MNIST & Select Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n",
      "The number of training samples is 12873, including sample 1: 6742, sample 3: 6131\n",
      "The number of testing samples is 2145, including digit 1: 1135, digit 3: 1010\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ed6a53f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ed694ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_dir = './MNIST'\n",
    "mnist_input = input_data.read_data_sets(data_dir, one_hot = True)\n",
    "#select two classes:digit '1' and '3'\n",
    "dataset_images = np.vstack((mnist_input.train.images, mnist_input.validation.images, mnist_input.test.images))\n",
    "dataset_labels = np.vstack((mnist_input.train.labels, mnist_input.validation.labels, mnist_input.test.labels))\n",
    "is_digit_1, is_digit_3 = (dataset_labels[:, 1] == 1.), (dataset_labels[:, 3] == 1.)\n",
    "selected_training = is_digit_1[:60000] + is_digit_3[:60000]\n",
    "selected_test = is_digit_1[60000:] + is_digit_3[60000:]\n",
    "selected = is_digit_1 + is_digit_3\n",
    "\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "training_x = np.round(dataset_images[:60000][selected_training], 0)\n",
    "training_y = dataset_labels[:60000][selected_training][:, [1, 3]]\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "test_x = np.round(dataset_images[60000:][selected_test], 0)\n",
    "test_y = dataset_labels[60000:][selected_test][:, [1, 3]]\n",
    "\n",
    "#output some useful imformation\n",
    "MSG = 'The number of training samples is {0}, including sample 1: {1}, sample 3: {2}' \n",
    "print MSG.format(training_x.shape[0], np.sum(is_digit_1[:60000]), np.sum(is_digit_3[:60000]))\n",
    "MSG = 'The number of testing samples is {0}, including digit 1: {1}, digit 3: {2}' \n",
    "print MSG.format(test_x.shape[0], np.sum(is_digit_1[60000:]), np.sum(is_digit_3[60000:]))\n",
    "\n",
    "for i in range(2):\n",
    "    plt.matshow(training_x[i, :].reshape(28, 28), vmin = 0., vmax = 1., cmap = plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#solver\n",
    "import utils\n",
    "input_dims = training_x.shape[1]\n",
    "hidden_dims =[32, 32]\n",
    "output_dims =2\n",
    "    \n",
    "params = {\n",
    "    'n_epoches':30,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':0.001,\n",
    "    'keep_prob': 0.6,\n",
    "    'input_dim' : input_dims,\n",
    "    'output_dim' : output_dims,\n",
    "    'hidden_dims': hidden_dims,\n",
    "    'adv_k' : 64,\n",
    "    'K' : 32,\n",
    "    'L' : 64\n",
    "}\n",
    "args = utils.ParamWrapper(params)\n",
    "\n",
    "#bais\n",
    "train_bais = np.array([[1., 1]], dtype = np.float32)\n",
    "test_bais = np.array([[1., 1]], dtype = np.float32)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "feed_forward_model = FFN_dae_lh_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the probability of corrupting hashing codes\n",
    "epsilon = 10.\n",
    "sigma = epsilon/input_dims\n",
    "mu = 0.0\n",
    "def random_noises(batch_size):\n",
    "    p = np.clip(np.random.randn()*sigma + mu, a_min= 0., a_max= 1.)\n",
    "    return np.random.binomial(1, p, (batch_size, args.L, args.K))\n",
    "\n",
    "def random_noises_vir(batch_size):\n",
    "    return np.zeros((batch_size, args.L, args.K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start DAE training...\n",
      "The iteation is 100, with training loss 0.216\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.206, with the best reconstruction loss is 0.206 and acheved at 100\n",
      "The iteation is 200, with training loss 0.201\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.194, with the best reconstruction loss is 0.194 and acheved at 200\n",
      "The iteation is 300, with training loss 0.192\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.181, with the best reconstruction loss is 0.181 and acheved at 300\n",
      "The iteation is 400, with training loss 0.183\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.173, with the best reconstruction loss is 0.173 and acheved at 400\n",
      "The iteation is 500, with training loss 0.177\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.155, with the best reconstruction loss is 0.155 and acheved at 500\n",
      "The iteation is 600, with training loss 0.165\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.146, with the best reconstruction loss is 0.146 and acheved at 600\n",
      "The iteation is 700, with training loss 0.16\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.14, with the best reconstruction loss is 0.14 and acheved at 700\n",
      "The iteation is 800, with training loss 0.157\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.138, with the best reconstruction loss is 0.138 and acheved at 800\n",
      "The iteation is 900, with training loss 0.156\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.136, with the best reconstruction loss is 0.136 and acheved at 900\n",
      "The iteation is 1000, with training loss 0.151\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.134, with the best reconstruction loss is 0.134 and acheved at 1000\n",
      "The iteation is 1100, with training loss 0.148\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.131, with the best reconstruction loss is 0.131 and acheved at 1100\n",
      "The iteation is 1200, with training loss 0.147\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.126, with the best reconstruction loss is 0.126 and acheved at 1200\n",
      "The iteation is 1300, with training loss 0.143\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.124, with the best reconstruction loss is 0.124 and acheved at 1300\n",
      "The iteation is 1400, with training loss 0.14\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.12, with the best reconstruction loss is 0.12 and acheved at 1400\n",
      "The iteation is 1500, with training loss 0.139\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.119, with the best reconstruction loss is 0.119 and acheved at 1500\n",
      "The iteation is 1600, with training loss 0.136\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.119, with the best reconstruction loss is 0.119 and acheved at 1600\n",
      "The iteation is 1700, with training loss 0.134\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.118, with the best reconstruction loss is 0.118 and acheved at 1700\n",
      "The iteation is 1800, with training loss 0.137\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.117, with the best reconstruction loss is 0.117 and acheved at 1800\n",
      "The iteation is 1900, with training loss 0.136\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.116, with the best reconstruction loss is 0.116 and acheved at 1900\n",
      "The iteation is 2000, with training loss 0.139\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.114, with the best reconstruction loss is 0.114 and acheved at 2000\n",
      "The iteation is 2100, with training loss 0.139\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.113, with the best reconstruction loss is 0.113 and acheved at 2100\n",
      "The iteation is 2200, with training loss 0.131\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.113, with the best reconstruction loss is 0.113 and acheved at 2200\n",
      "The iteation is 2300, with training loss 0.132\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.112, with the best reconstruction loss is 0.112 and acheved at 2300\n",
      "The iteation is 2400, with training loss 0.134\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.111, with the best reconstruction loss is 0.111 and acheved at 2400\n",
      "The iteation is 2500, with training loss 0.13\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.111, with the best reconstruction loss is 0.111 and acheved at 2500\n",
      "The iteation is 2600, with training loss 0.137\n",
      "\t the test loss is 0.111, with the best reconstruction loss is 0.111 and acheved at 2500\n",
      "The iteation is 2700, with training loss 0.129\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.108, with the best reconstruction loss is 0.108 and acheved at 2700\n",
      "The iteation is 2800, with training loss 0.133\n",
      "\t the test loss is 0.108, with the best reconstruction loss is 0.108 and acheved at 2700\n",
      "The iteation is 2900, with training loss 0.127\n",
      "the parameters of model saved at ./MNIST/model/jid/jid.ckpt\n",
      "\t the test loss is 0.106, with the best reconstruction loss is 0.106 and acheved at 2900\n",
      "The iteation is 3000, with training loss 0.127\n",
      "\t the test loss is 0.107, with the best reconstruction loss is 0.106 and acheved at 2900\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "saver_dae = tf.train.Saver(feed_forward_model.share_vars)\n",
    "model_dir = './MNIST/model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_jid_dir = os.path.join(model_dir, \"jid/\")\n",
    "if not os.path.exists(model_jid_dir):\n",
    "    os.makedirs(model_jid_dir)\n",
    "    \n",
    "model_save_path = os.path.join(model_jid_dir, \"jid.ckpt\")\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "_shuffle_idx = np.arange(0, training_x.shape[0])\n",
    "np.random.shuffle(_shuffle_idx)\n",
    "display_step = 100\n",
    "print(\"Start DAE training...\")\n",
    "best_recon_loss = 5.\n",
    "best_iteration = 0\n",
    "with sess.as_default():\n",
    "    sess.run(init)\n",
    "    n_batches = training_x.shape[0] // args.batch_size\n",
    "    avg_losses = []\n",
    "    for epoch in range(args.n_epoches): #increaing the epochs could improve the resistance\n",
    "        for mini_idx in range(n_batches):\n",
    "            iterations = mini_idx + 1 + epoch * n_batches\n",
    "            start_i = mini_idx * args.batch_size \n",
    "            end_i = start_i + args.batch_size \n",
    "            if end_i > training_x.shape[0]:\n",
    "                end_i = training_x.shape[0]\n",
    "            _feed_dict = {\n",
    "                feed_forward_model.x: training_x[_shuffle_idx[start_i:end_i]],\n",
    "                feed_forward_model.noises: random_noises(end_i - start_i),\n",
    "                feed_forward_model.is_training: True\n",
    "            }\n",
    "            _, loss = sess.run([feed_forward_model.dae_optimizer, feed_forward_model.dae_loss], feed_dict = _feed_dict)\n",
    "            \n",
    "            if iterations % display_step == 0:\n",
    "                MSG1 = \"The iteation is {0}, with training loss {1:.3}\"\n",
    "                print(MSG1.format(iterations, loss))\n",
    "                \n",
    "                test_noise = random_noises_vir(test_x.shape[0])\n",
    "                _feed_dict = {\n",
    "                    feed_forward_model.x: test_x,\n",
    "                    feed_forward_model.noises: test_noise,\n",
    "                    feed_forward_model.is_training: False\n",
    "                }\n",
    "                loss2 = sess.run(feed_forward_model.dae_loss, feed_dict = _feed_dict)\n",
    "                if loss2 < best_recon_loss:\n",
    "                    best_recon_loss = loss2\n",
    "                    best_iteration = iterations\n",
    "                    saver_dae.save(sess, model_save_path)\n",
    "                    print(\"the parameters of model saved at %s\" % model_save_path)\n",
    "                MSG2 = \"\\t the test loss is {0:.3}, with the best reconstruction loss is {1:.3} and acheved at {2}\"\n",
    "                print(MSG2.format(loss2, best_recon_loss, best_iteration))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteation is 100, with training loss 0.0371 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0254 and test accuracy is 0.99207\n",
      "The iteation is 200, with training loss 0.0458 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0245 and test accuracy is 0.99207\n",
      "The iteation is 300, with training loss 0.031 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0227 and test accuracy is 0.99161\n",
      "The iteation is 400, with training loss 0.0139 and batch accuracy 1.0\n",
      "\t the test loss is 0.0218 and test accuracy is 0.99207\n",
      "The iteation is 500, with training loss 0.0279 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0229 and test accuracy is 0.99114\n",
      "The iteation is 600, with training loss 0.0273 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0224 and test accuracy is 0.99114\n",
      "The iteation is 700, with training loss 0.00856 and batch accuracy 1.0\n",
      "\t the test loss is 0.0199 and test accuracy is 0.99301\n",
      "The iteation is 800, with training loss 0.0268 and batch accuracy 0.9922\n",
      "\t the test loss is 0.021 and test accuracy is 0.99114\n",
      "The iteation is 900, with training loss 0.012 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0214 and test accuracy is 0.99254\n",
      "The iteation is 1000, with training loss 0.0147 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0215 and test accuracy is 0.99207\n",
      "The iteation is 1100, with training loss 0.0219 and batch accuracy 0.9922\n",
      "\t the test loss is 0.021 and test accuracy is 0.99114\n",
      "The iteation is 1200, with training loss 0.0153 and batch accuracy 1.0\n",
      "\t the test loss is 0.0198 and test accuracy is 0.99441\n",
      "The iteation is 1300, with training loss 0.0372 and batch accuracy 0.9844\n",
      "\t the test loss is 0.022 and test accuracy is 0.99114\n",
      "The iteation is 1400, with training loss 0.0168 and batch accuracy 1.0\n",
      "\t the test loss is 0.0213 and test accuracy is 0.99301\n",
      "The iteation is 1500, with training loss 0.0238 and batch accuracy 0.9922\n",
      "\t the test loss is 0.021 and test accuracy is 0.99161\n",
      "The iteation is 1600, with training loss 0.0192 and batch accuracy 1.0\n",
      "\t the test loss is 0.022 and test accuracy is 0.99114\n",
      "The iteation is 1700, with training loss 0.00722 and batch accuracy 1.0\n",
      "\t the test loss is 0.0205 and test accuracy is 0.99347\n",
      "The iteation is 1800, with training loss 0.0144 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0215 and test accuracy is 0.99301\n",
      "The iteation is 1900, with training loss 0.0148 and batch accuracy 1.0\n",
      "\t the test loss is 0.0213 and test accuracy is 0.99254\n",
      "The iteation is 2000, with training loss 0.0072 and batch accuracy 1.0\n",
      "\t the test loss is 0.0217 and test accuracy is 0.99254\n",
      "The iteation is 2100, with training loss 0.00933 and batch accuracy 1.0\n",
      "\t the test loss is 0.022 and test accuracy is 0.99254\n",
      "The iteation is 2200, with training loss 0.00926 and batch accuracy 1.0\n",
      "\t the test loss is 0.0218 and test accuracy is 0.99301\n",
      "The iteation is 2300, with training loss 0.00802 and batch accuracy 1.0\n",
      "\t the test loss is 0.0229 and test accuracy is 0.99254\n",
      "The iteation is 2400, with training loss 0.0236 and batch accuracy 1.0\n",
      "\t the test loss is 0.02 and test accuracy is 0.99207\n",
      "The iteation is 2500, with training loss 0.0128 and batch accuracy 1.0\n",
      "\t the test loss is 0.0182 and test accuracy is 0.99394\n",
      "The iteation is 2600, with training loss 0.0149 and batch accuracy 1.0\n",
      "\t the test loss is 0.0194 and test accuracy is 0.99254\n",
      "The iteation is 2700, with training loss 0.00653 and batch accuracy 1.0\n",
      "\t the test loss is 0.0185 and test accuracy is 0.99254\n",
      "The iteation is 2800, with training loss 0.0123 and batch accuracy 1.0\n",
      "\t the test loss is 0.0188 and test accuracy is 0.99301\n",
      "The iteation is 2900, with training loss 0.0129 and batch accuracy 1.0\n",
      "\t the test loss is 0.0186 and test accuracy is 0.99534\n",
      "The iteation is 3000, with training loss 0.00659 and batch accuracy 1.0\n",
      "\t the test loss is 0.0228 and test accuracy is 0.99254\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    best_acc= 0.\n",
    "    iterations = 0\n",
    "    display = 100\n",
    "    n_batches = training_x.shape[0] // args.batch_size\n",
    "    for epoch in range(args.n_epoches):\n",
    "        for mini_idx in range(n_batches):\n",
    "            iterations = mini_idx + 1 + epoch * n_batches\n",
    "            start_i = mini_idx * args.batch_size \n",
    "            end_i = start_i + args.batch_size \n",
    "            if end_i > training_x.shape[0]:\n",
    "                end_i = training_x.shape[0]\n",
    "            _feed_dict = {\n",
    "                feed_forward_model.x: training_x[_shuffle_idx[start_i:end_i]],\n",
    "                feed_forward_model.noises: random_noises_vir(end_i - start_i),\n",
    "                feed_forward_model.y: training_y[_shuffle_idx[start_i:end_i]],\n",
    "                feed_forward_model.bias: train_bais,\n",
    "                feed_forward_model.is_training: True,\n",
    "                feed_forward_model.is_adv_training: False\n",
    "            }\n",
    "            _, loss, acc = sess.run([feed_forward_model.update_clf, feed_forward_model.clf_loss, feed_forward_model.acc], feed_dict = _feed_dict)\n",
    "            \n",
    "            if iterations % display == 0:\n",
    "                _feed_dict = {\n",
    "                    feed_forward_model.x: test_x,\n",
    "                    feed_forward_model.noises: random_noises_vir(test_x.shape[0]),\n",
    "                    feed_forward_model.y: test_y,\n",
    "                    feed_forward_model.bias: test_bais,\n",
    "                    feed_forward_model.is_training: False,\n",
    "                    feed_forward_model.is_adv_training: False\n",
    "                }\n",
    "                loss2, acc2 = sess.run([feed_forward_model.clf_loss, feed_forward_model.acc], feed_dict = _feed_dict)\n",
    "                \n",
    "                MSG1 = \"The iteation is {0}, with training loss {1:.3} and batch accuracy {2:.4}\"\n",
    "                MSG2 = \"\\t the test loss is {0:.3} and test accuracy is {1:.5}\"\n",
    "                print MSG1.format(iterations, loss, acc)\n",
    "                print MSG2.format(loss2, acc2)\n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================Test Adversarial Examples================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load adversarial examples\n",
    "save_dir = \"./MNIST\"\n",
    "\n",
    "save_path1 = os.path.join(save_dir, 'clean.data')\n",
    "save_path2 = os.path.join(save_dir, 'fgsm_10.data')\n",
    "save_path3 = os.path.join(save_dir, 'fgsm_20.data')\n",
    "save_path4 = os.path.join(save_dir, 'fgsm_50.data')\n",
    "save_path5 = os.path.join(save_dir, 'fgsm_100.data')\n",
    "save_path_label = os.path.join(save_dir, 'clean.label')\n",
    "\n",
    "save_path1_opt = os.path.join(save_dir, 'opt_5.data')\n",
    "save_path2_opt = os.path.join(save_dir, 'opt_10.data')\n",
    "save_path3_opt = os.path.join(save_dir, 'opt_20.data')\n",
    "\n",
    "samples = utils.readdata_np(save_path1)\n",
    "labels =  utils.readdata_np(save_path_label)\n",
    "adv_examples_fgm_10 =  utils.readdata_np(save_path2)\n",
    "adv_examples_fgm_20 =  utils.readdata_np(save_path3)\n",
    "adv_examples_fgm_50 =  utils.readdata_np(save_path4)\n",
    "adv_examples_fgm_100 =  utils.readdata_np(save_path5)\n",
    "\n",
    "adv_examples_opt_5 =  utils.readdata_np(save_path1_opt)\n",
    "adv_examples_opt_10 =  utils.readdata_np(save_path2_opt)\n",
    "adv_examples_opt_20 =  utils.readdata_np(save_path3_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_adv_sampels(sess, adv_samples, labels):\n",
    "    _feed_dicts = {\n",
    "        feed_forward_model.x: adv_samples,\n",
    "        feed_forward_model.noises: random_noises_vir(adv_samples.shape[0]),\n",
    "        feed_forward_model.y: labels,\n",
    "        feed_forward_model.bias:test_bais,\n",
    "        feed_forward_model.is_training: False,\n",
    "        feed_forward_model.is_adv_training: False\n",
    "    }\n",
    "    return sess.run(feed_forward_model.acc, feed_dict = _feed_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on clean sampels is 1.000, vs that on adversarial exampels generated by fgm is 1.000, 1.000, 1.000, 0.970\n",
      "The accuracy on adversarial examples generated by optimization method is 1.000, 1.000, 1.000\n"
     ]
    }
   ],
   "source": [
    "acc_clean = evaluate_adv_sampels(sess, samples, labels)\n",
    "acc_fgs_10 = evaluate_adv_sampels(sess, adv_examples_fgm_10, labels)\n",
    "acc_fgs_20 = evaluate_adv_sampels(sess, adv_examples_fgm_20, labels)\n",
    "acc_fgs_50 = evaluate_adv_sampels(sess, adv_examples_fgm_50, labels)\n",
    "acc_fgs_100 = evaluate_adv_sampels(sess, adv_examples_fgm_100, labels)\n",
    "acc_opt_5 = evaluate_adv_sampels(sess, adv_examples_opt_5, labels)\n",
    "acc_opt_10 = evaluate_adv_sampels(sess, adv_examples_opt_10, labels)\n",
    "acc_opt_20 = evaluate_adv_sampels(sess, adv_examples_opt_20, labels)\n",
    "print(\"The accuracy on clean sampels is %.3f, vs that on adversarial exampels generated by fgm is %.3f, %.3f, %.3f, %.3f\" \\\n",
    "      % (acc_clean, acc_fgs_10, acc_fgs_20, acc_fgs_50, acc_fgs_100))\n",
    "print(\"The accuracy on adversarial examples generated by optimization method is %.3f, %.3f, %.3f\" % \\\n",
    "      (acc_opt_5, acc_opt_10, acc_opt_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
