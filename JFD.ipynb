{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lclhome/dli/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from graphs import lhrf_model, FFN_lhrf_dae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load MNIST & Select Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n",
      "The number of training samples is 12873, including sample 1: 6742, sample 3: 6131\n",
      "The number of testing samples is 2145, including digit 1: 1135, digit 3: 1010\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb904552e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb904450f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_dir = './MNIST'\n",
    "mnist_input = input_data.read_data_sets(data_dir, one_hot = True)\n",
    "#select two classes:digit '1' and '3'\n",
    "dataset_images = np.vstack((mnist_input.train.images, mnist_input.validation.images, mnist_input.test.images))\n",
    "dataset_labels = np.vstack((mnist_input.train.labels, mnist_input.validation.labels, mnist_input.test.labels))\n",
    "is_digit_1, is_digit_3 = (dataset_labels[:, 1] == 1.), (dataset_labels[:, 3] == 1.)\n",
    "selected_training = is_digit_1[:60000] + is_digit_3[:60000]\n",
    "selected_test = is_digit_1[60000:] + is_digit_3[60000:]\n",
    "selected = is_digit_1 + is_digit_3\n",
    "\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "training_x = np.round(dataset_images[:60000][selected_training], 0)\n",
    "training_y = dataset_labels[:60000][selected_training][:, [1, 3]]\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "test_x = np.round(dataset_images[60000:][selected_test], 0)\n",
    "test_y = dataset_labels[60000:][selected_test][:, [1, 3]]\n",
    "\n",
    "#output some useful imformation\n",
    "MSG = 'The number of training samples is {0}, including sample 1: {1}, sample 3: {2}' \n",
    "print MSG.format(training_x.shape[0], np.sum(is_digit_1[:60000]), np.sum(is_digit_3[:60000]))\n",
    "MSG = 'The number of testing samples is {0}, including digit 1: {1}, digit 3: {2}' \n",
    "print MSG.format(test_x.shape[0], np.sum(is_digit_1[60000:]), np.sum(is_digit_3[60000:]))\n",
    "\n",
    "for i in range(2):\n",
    "    plt.matshow(training_x[i, :].reshape(28, 28), vmin = 0., vmax = 1., cmap = plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "input_dims = training_x.shape[1]\n",
    "hidden_dims =[32, 32]\n",
    "output_dims =2\n",
    "model_dir = os.path.join(data_dir, \"model/\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "model_save_dir = os.path.join(model_dir, \"jfd/\")\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.mkdir(model_save_dir)\n",
    "    \n",
    "_config = {\n",
    "    'save_dir': model_save_dir,\n",
    "    'n_epochs':30,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':0.001,\n",
    "    'keep_prob': 0.6,\n",
    "    'input_dim' : input_dims,\n",
    "    'output_dim' : output_dims,\n",
    "    'hidden_dims': hidden_dims,\n",
    "    'K' : 16,\n",
    "    'L' : 16\n",
    "}\n",
    "args = utils.ParamWrapper(_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Learn to hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 7 7 6 3 6 4 3 4]\n",
      "[11  6 14 13 11  4  6  4  3 13]\n",
      "[ 4  7  3 14 10 11 14  4  7 13]\n",
      "[13 13 11  6  3  7 13  4 11  3]\n",
      "[ 3  6 10  6  4  7  6 13  3 14]\n",
      "[ 3 11  6  4  4  6  6  4 10 11]\n",
      "[10  7  6  4  6  6 11  4 11  6]\n",
      "[14  7  7  7 13 10 14  6  7 14]\n",
      "[ 6  7  6  6 14 14  3  4 11  3]\n",
      "[ 4  4 14  7 11  6 11  6  3 11]\n",
      "[11 14 14  7  3  7 13 14  4  6]\n",
      "[ 7  4  7  6  6  6  3  3 14  4]\n",
      "[ 3  6  7  4  4  3  6  3 13  7]\n",
      "[ 6  6 13  3  6 10  4  3 11 13]\n",
      "[11 11  7  4 10 13  6  4  3  3]\n",
      "[ 3  6 10  4 14  3 10 14  3  7]\n"
     ]
    }
   ],
   "source": [
    "#learning\n",
    "lh_rf = lhrf_model(args, training_x, training_y[:,0])\n",
    "#hashing\n",
    "trainX_rps = lh_rf.learning_hashing_by_rf.hashing_func(training_x)\n",
    "testX_rps = lh_rf.learning_hashing_by_rf.hashing_func(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Config DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepra data\n",
    "train_input = utils.DataProducer(trainX_rps, training_y, args.batch_size, args.n_epochs, \"train\")\n",
    "test_input = utils.DataProducer(testX_rps, test_y, args.batch_size, args.n_epochs, \"test\")\n",
    "\n",
    "args.input_dim = list(trainX_rps.shape[1:])\n",
    "args.input_dim.append(training_x.shape[1])\n",
    "#bais\n",
    "train_bias = np.array([[1., 1]], dtype = np.float32)\n",
    "test_bias = np.array([[1., 1]], dtype = np.float32)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = FFN_lhrf_dae_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the probability of corrupting hashing codes\n",
    "epsilon = 10.\n",
    "sigma = epsilon/input_dims\n",
    "mu = 0.0\n",
    "def random_noises_dae(batch_size, onehot_dim, input_dim):\n",
    "    prob = np.clip(np.random.randn() * sigma + mu, a_min = 0., a_max = 1.)\n",
    "    return np.random.binomial(1, prob, (batch_size, args.L, onehot_dim))\n",
    "\n",
    "def random_noises_vir(batch_size,  onehot_dim):\n",
    "    return np.zeros((batch_size, args.L, onehot_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 100, epoch 1/30, minibatch 100/101, with training loss 0.13916 and test loss 0.14274.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 200, epoch 2/30, minibatch 99/101, with training loss 0.13484 and test loss 0.13462.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 300, epoch 3/30, minibatch 98/101, with training loss 0.11509 and test loss 0.11219.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 400, epoch 4/30, minibatch 97/101, with training loss 0.11403 and test loss 0.10945.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 500, epoch 5/30, minibatch 96/101, with training loss 0.12199 and test loss 0.10748.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 600, epoch 6/30, minibatch 95/101, with training loss 0.10353 and test loss 0.10467.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 700, epoch 7/30, minibatch 94/101, with training loss 0.11036 and test loss 0.10141.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 800, epoch 8/30, minibatch 93/101, with training loss 0.11948 and test loss 0.098499.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 900, epoch 9/30, minibatch 92/101, with training loss 0.092501 and test loss 0.095947.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1000, epoch 10/30, minibatch 91/101, with training loss 0.099823 and test loss 0.094658.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1100, epoch 11/30, minibatch 90/101, with training loss 0.10405 and test loss 0.093243.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1200, epoch 12/30, minibatch 89/101, with training loss 0.089971 and test loss 0.091955.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1300, epoch 13/30, minibatch 88/101, with training loss 0.1022 and test loss 0.089702.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1400, epoch 14/30, minibatch 87/101, with training loss 0.092846 and test loss 0.087947.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1500, epoch 15/30, minibatch 86/101, with training loss 0.094908 and test loss 0.087406.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1600, epoch 16/30, minibatch 85/101, with training loss 0.10145 and test loss 0.08682.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1700, epoch 17/30, minibatch 84/101, with training loss 0.095989 and test loss 0.085993.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1800, epoch 18/30, minibatch 83/101, with training loss 0.090039 and test loss 0.085509.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 1900, epoch 19/30, minibatch 82/101, with training loss 0.092585 and test loss 0.084798.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2000, epoch 20/30, minibatch 81/101, with training loss 0.09394 and test loss 0.083776.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2100, epoch 21/30, minibatch 80/101, with training loss 0.08995 and test loss 0.082722.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2200, epoch 22/30, minibatch 79/101, with training loss 0.094677 and test loss 0.081507.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2300, epoch 23/30, minibatch 78/101, with training loss 0.092432 and test loss 0.080057.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2400, epoch 24/30, minibatch 77/101, with training loss 0.098167 and test loss 0.07798.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2500, epoch 25/30, minibatch 76/101, with training loss 0.096309 and test loss 0.077131.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2600, epoch 26/30, minibatch 75/101, with training loss 0.090829 and test loss 0.076547.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2700, epoch 27/30, minibatch 74/101, with training loss 0.081386 and test loss 0.075947.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2800, epoch 28/30, minibatch 73/101, with training loss 0.08393 and test loss 0.075176.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 2900, epoch 29/30, minibatch 72/101, with training loss 0.091155 and test loss 0.07448.\n",
      "the parameters of model saved at ./MNIST/model/jfd/jfd.ckpt\n",
      "The iteration is 3000, epoch 30/30, minibatch 71/101, with training loss 0.083904 and test loss 0.074007.\n"
     ]
    }
   ],
   "source": [
    "#dae training\n",
    "saver_dae = tf.train.Saver(model.share_vars)\n",
    "model_save_path = os.path.join(model_save_dir, \"jfd.ckpt\")\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "with sess.as_default():\n",
    "    sess.run(init)\n",
    "    best_recon_loss = 5.\n",
    "    best_iteration = 0\n",
    "    #start DAE training\n",
    "    for epoch in range(args.n_epochs):\n",
    "            train_input.reset_cursor()\n",
    "            for mini_batch, trainX_batch, trainy_batch in train_input.next_batch():\n",
    "                train_dict = {model.x : trainX_batch,\n",
    "                              model.noises: random_noises_dae(trainX_batch.shape[0], trainX_batch.shape[2], args.input_dim[2]),\n",
    "                              model.is_training: True\n",
    "                                  }\n",
    "                _, dae_train_loss = sess.run([model.dae_optimizer, model.dae_loss],\n",
    "                                                feed_dict= train_dict)\n",
    "\n",
    "                \n",
    "                iterations = epoch * train_input.mini_batches + mini_batch + 1\n",
    "                if iterations % 100 == 0:\n",
    "                    test_input.reset_cursor()\n",
    "                    dae_test_loss = [sess.run(model.dae_loss, feed_dict = {model.x: testX_batch,\n",
    "                                                                   model.noises: random_noises_vir(testX_batch.shape[0], testX_batch.shape[2]),\n",
    "                                                                   model.is_training: False}) \\\n",
    "                                 for [_, testX_batch, testy_batch] in test_input.next_batch()\n",
    "                    ]\n",
    "\n",
    "                    if np.mean(dae_test_loss) < best_recon_loss:\n",
    "                        #save model parameters\n",
    "                        best_recon_loss = np.mean(dae_test_loss)\n",
    "                        save_model_path = os.path.join(model_save_path + \"model.ckpt\")\n",
    "                        saver_dae.save(sess, save_model_path)\n",
    "                        print(\"the parameters of model saved at %s\" % model_save_path)\n",
    "                    MSG = \"The iteration is {0}, epoch {1}/{2}, minibatch {3}/{4}, with training loss {5:.5} and test loss {6:.5}.\"\n",
    "                    print(MSG.format(iterations, epoch + 1, args.n_epochs, mini_batch + 1, train_input.mini_batches, dae_train_loss, np.mean(dae_test_loss)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./MNIST/model/jfd/jfd.ckptmodel.ckpt\n",
      "The iteration is 100, epoch 1/30, minibatch 100/101, with training loss 0.063211, training accuracy 0.98438 and test accuracy 0.99235.\n",
      "\tThe best test accuracy is 0.99235, which achieves at iteration 100, wiht test accuracy 0.99235\n",
      "The iteration is 200, epoch 2/30, minibatch 99/101, with training loss 0.023903, training accuracy 0.98438 and test accuracy 0.99434.\n",
      "\tThe best test accuracy is 0.99434, which achieves at iteration 200, wiht test accuracy 0.99434\n",
      "The iteration is 300, epoch 3/30, minibatch 98/101, with training loss 0.11921, training accuracy 0.96094 and test accuracy 0.99449.\n",
      "\tThe best test accuracy is 0.99449, which achieves at iteration 300, wiht test accuracy 0.99449\n",
      "The iteration is 400, epoch 4/30, minibatch 97/101, with training loss 0.012267, training accuracy 1.0 and test accuracy 0.99403.\n",
      "The iteration is 500, epoch 5/30, minibatch 96/101, with training loss 0.025104, training accuracy 1.0 and test accuracy 0.99388.\n",
      "The iteration is 600, epoch 6/30, minibatch 95/101, with training loss 0.012408, training accuracy 1.0 and test accuracy 0.9954.\n",
      "\tThe best test accuracy is 0.9954, which achieves at iteration 600, wiht test accuracy 0.9954\n",
      "The iteration is 700, epoch 7/30, minibatch 94/101, with training loss 0.028658, training accuracy 0.98438 and test accuracy 0.99494.\n",
      "The iteration is 800, epoch 8/30, minibatch 93/101, with training loss 0.011817, training accuracy 0.99219 and test accuracy 0.99434.\n",
      "The iteration is 900, epoch 9/30, minibatch 92/101, with training loss 0.0039936, training accuracy 1.0 and test accuracy 0.9954.\n",
      "The iteration is 1000, epoch 10/30, minibatch 91/101, with training loss 0.0024774, training accuracy 1.0 and test accuracy 0.99494.\n",
      "The iteration is 1100, epoch 11/30, minibatch 90/101, with training loss 0.0068493, training accuracy 1.0 and test accuracy 0.9954.\n",
      "The iteration is 1200, epoch 12/30, minibatch 89/101, with training loss 0.001797, training accuracy 1.0 and test accuracy 0.9954.\n",
      "The iteration is 1300, epoch 13/30, minibatch 88/101, with training loss 0.0077884, training accuracy 1.0 and test accuracy 0.9954.\n",
      "The iteration is 1400, epoch 14/30, minibatch 87/101, with training loss 0.038859, training accuracy 0.98438 and test accuracy 0.99586.\n",
      "\tThe best test accuracy is 0.99586, which achieves at iteration 1400, wiht test accuracy 0.99586\n",
      "The iteration is 1500, epoch 15/30, minibatch 86/101, with training loss 0.019846, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 1600, epoch 16/30, minibatch 85/101, with training loss 0.016985, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 1700, epoch 17/30, minibatch 84/101, with training loss 0.014284, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 1800, epoch 18/30, minibatch 83/101, with training loss 0.027187, training accuracy 0.98438 and test accuracy 0.99586.\n",
      "The iteration is 1900, epoch 19/30, minibatch 82/101, with training loss 0.025838, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 2000, epoch 20/30, minibatch 81/101, with training loss 0.014659, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 2100, epoch 21/30, minibatch 80/101, with training loss 0.034471, training accuracy 0.97656 and test accuracy 0.99632.\n",
      "\tThe best test accuracy is 0.99632, which achieves at iteration 2100, wiht test accuracy 0.99632\n",
      "The iteration is 2200, epoch 22/30, minibatch 79/101, with training loss 0.0068202, training accuracy 1.0 and test accuracy 0.99586.\n",
      "The iteration is 2300, epoch 23/30, minibatch 78/101, with training loss 0.056998, training accuracy 0.96875 and test accuracy 0.99586.\n",
      "The iteration is 2400, epoch 24/30, minibatch 77/101, with training loss 0.024772, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 2500, epoch 25/30, minibatch 76/101, with training loss 0.023823, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 2600, epoch 26/30, minibatch 75/101, with training loss 0.016219, training accuracy 0.99219 and test accuracy 0.99632.\n",
      "The iteration is 2700, epoch 27/30, minibatch 74/101, with training loss 0.0038012, training accuracy 1.0 and test accuracy 0.99586.\n",
      "The iteration is 2800, epoch 28/30, minibatch 73/101, with training loss 0.013507, training accuracy 0.99219 and test accuracy 0.99586.\n",
      "The iteration is 2900, epoch 29/30, minibatch 72/101, with training loss 0.0072828, training accuracy 1.0 and test accuracy 0.99586.\n",
      "The iteration is 3000, epoch 30/30, minibatch 71/101, with training loss 0.044399, training accuracy 0.98438 and test accuracy 0.99632.\n"
     ]
    }
   ],
   "source": [
    "#optimize classifier\n",
    "#saver = tf.train.Saver()\n",
    "with sess.as_default():\n",
    "    saver_dae.restore(sess, save_model_path)\n",
    "    best_iter = 0\n",
    "    best_acc = 0.\n",
    "    for epoch in range(args.n_epochs):\n",
    "        train_input.reset_cursor()\n",
    "        for mini_batch, trainX_batch, trainy_batch in train_input.next_batch():\n",
    "            train_dict = {model.x : trainX_batch,\n",
    "                          model.noises:random_noises_vir(trainX_batch.shape[0], trainX_batch.shape[2]),\n",
    "                          model.y : trainy_batch,\n",
    "                          model.bias: train_bias,\n",
    "                          model.is_training: True\n",
    "                          }\n",
    "            _, train_loss, train_acc = sess.run([model.update, model.loss, model.acc],\n",
    "                                                feed_dict= train_dict)\n",
    "            iterations = epoch * train_input.mini_batches + mini_batch + 1\n",
    "            if iterations % 100 == 0:\n",
    "                test_input.reset_cursor()\n",
    "                test_accs = [sess.run(model.acc, feed_dict = {model.x: testX_batch,\n",
    "                                                              model.noises:random_noises_vir(testX_batch.shape[0], testX_batch.shape[2]),\n",
    "                                                              model.y: testy_batch,\n",
    "                                                              model.bias: test_bias,\n",
    "                                                              model.is_training: False}) \\\n",
    "                             for [_, testX_batch, testy_batch] in test_input.next_batch()\n",
    "                ]\n",
    "                test_acc = np.mean(test_accs)\n",
    "\n",
    "                MSG = \"The iteration is {0}, epoch {1}/{2}, minibatch {3}/{4}, with training loss {5:.5}, training accuracy {6:.5} and test accuracy {7:.5}.\"\n",
    "                print(MSG.format(iterations, epoch + 1, args.n_epochs, mini_batch + 1, train_input.mini_batches, train_loss, train_acc, test_acc))\n",
    "                if best_acc < test_acc:\n",
    "                    best_iter = iterations\n",
    "                    best_acc = test_acc\n",
    "                    MSG = \"\\tThe best test accuracy is {0:.5}, which achieves at iteration {1}, wiht test accuracy {2:.5}\"\n",
    "                    print(MSG.format(best_acc, best_iter, test_acc))\n",
    "                    #save model parameters\n",
    "                    #saver.save(sess, save_model_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================Test Adversarial Examples================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load adversarial examples\n",
    "save_dir = \"./MNIST\"\n",
    "\n",
    "save_path1 = os.path.join(save_dir, 'clean.data')\n",
    "save_path2 = os.path.join(save_dir, 'fgsm_10.data')\n",
    "save_path3 = os.path.join(save_dir, 'fgsm_20.data')\n",
    "save_path4 = os.path.join(save_dir, 'fgsm_50.data')\n",
    "save_path5 = os.path.join(save_dir, 'fgsm_100.data')\n",
    "save_path_label = os.path.join(save_dir, 'clean.label')\n",
    "\n",
    "save_path1_opt = os.path.join(save_dir, 'opt_5.data')\n",
    "save_path2_opt = os.path.join(save_dir, 'opt_10.data')\n",
    "save_path3_opt = os.path.join(save_dir, 'opt_20.data')\n",
    "\n",
    "samples = utils.readdata_np(save_path1)\n",
    "labels =  utils.readdata_np(save_path_label)\n",
    "adv_examples_fgm_10 =  utils.readdata_np(save_path2)\n",
    "adv_examples_fgm_20 =  utils.readdata_np(save_path3)\n",
    "adv_examples_fgm_50 =  utils.readdata_np(save_path4)\n",
    "adv_examples_fgm_100 =  utils.readdata_np(save_path5)\n",
    "\n",
    "adv_examples_opt_5 =  utils.readdata_np(save_path1_opt)\n",
    "adv_examples_opt_10 =  utils.readdata_np(save_path2_opt)\n",
    "adv_examples_opt_20 =  utils.readdata_np(save_path3_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_adv_sampels(sess, adv_samples, labels):\n",
    "    adv_hashing = lh_rf.learning_hashing_by_rf.hashing_func(adv_samples)\n",
    "    eva_dict = {model.x : adv_hashing,\n",
    "                model.y : labels,\n",
    "                model.bias: test_bias,\n",
    "                model.is_training: False\n",
    "                }\n",
    "    return sess.run(model.acc,feed_dict= train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on clean sampels is 1.000, vs that on adversarial exampels generated by fgm is 1.000, 1.000, 1.000, 1.000\n",
      "The accuracy on adversarial examples generated by optimization method is 1.000, 1.000, 1.000\n"
     ]
    }
   ],
   "source": [
    "acc_clean = evaluate_adv_sampels(sess, samples, labels)\n",
    "acc_fgs_10 = evaluate_adv_sampels(sess, adv_examples_fgm_10, labels)\n",
    "acc_fgs_20 = evaluate_adv_sampels(sess, adv_examples_fgm_20, labels)\n",
    "acc_fgs_50 = evaluate_adv_sampels(sess, adv_examples_fgm_50, labels)\n",
    "acc_fgs_100 = evaluate_adv_sampels(sess, adv_examples_fgm_100, labels)\n",
    "acc_opt_5 = evaluate_adv_sampels(sess, adv_examples_opt_5, labels)\n",
    "acc_opt_10 = evaluate_adv_sampels(sess, adv_examples_opt_10, labels)\n",
    "acc_opt_20 = evaluate_adv_sampels(sess, adv_examples_opt_20, labels)\n",
    "print(\"The accuracy on clean sampels is %.3f, vs that on adversarial exampels generated by fgm is %.3f, %.3f, %.3f, %.3f\" \\\n",
    "      % (acc_clean, acc_fgs_10, acc_fgs_20, acc_fgs_50, acc_fgs_100))\n",
    "print(\"The accuracy on adversarial examples generated by optimization method is %.3f, %.3f, %.3f\" % \\\n",
    "      (acc_opt_5, acc_opt_10, acc_opt_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
