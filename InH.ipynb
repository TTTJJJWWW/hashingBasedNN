{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lclhome/dli/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import csv\n",
    "import cPickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from graphs import FFN_lh_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load MNIST & Select Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n",
      "The number of training samples is 12873, including sample 1: 6742, sample 3: 6131\n",
      "The number of testing samples is 2145, including digit 1: 1135, digit 3: 1010\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc44093f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc4072fe90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_dir = './MNIST'\n",
    "mnist_input = input_data.read_data_sets(data_dir, one_hot = True)\n",
    "#select two classes:digit '1' and '3'\n",
    "dataset_images = np.vstack((mnist_input.train.images, mnist_input.validation.images, mnist_input.test.images))\n",
    "dataset_labels = np.vstack((mnist_input.train.labels, mnist_input.validation.labels, mnist_input.test.labels))\n",
    "is_digit_1, is_digit_3 = (dataset_labels[:, 1] == 1.), (dataset_labels[:, 3] == 1.)\n",
    "selected_training = is_digit_1[:60000] + is_digit_3[:60000]\n",
    "selected_test = is_digit_1[60000:] + is_digit_3[60000:]\n",
    "selected = is_digit_1 + is_digit_3\n",
    "\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "training_x = np.round(dataset_images[:60000][selected_training], 0)\n",
    "training_y = dataset_labels[:60000][selected_training][:, [1, 3]]\n",
    "#discretize the attributes, and round the pixel to be 0 or 1\n",
    "test_x = np.round(dataset_images[60000:][selected_test], 0)\n",
    "test_y = dataset_labels[60000:][selected_test][:, [1, 3]]\n",
    "\n",
    "#output some useful imformation\n",
    "MSG = 'The number of training samples is {0}, including sample 1: {1}, sample 3: {2}' \n",
    "print MSG.format(training_x.shape[0], np.sum(is_digit_1[:60000]), np.sum(is_digit_3[:60000]))\n",
    "MSG = 'The number of testing samples is {0}, including digit 1: {1}, digit 3: {2}' \n",
    "print MSG.format(test_x.shape[0], np.sum(is_digit_1[60000:]), np.sum(is_digit_3[60000:]))\n",
    "\n",
    "for i in range(2):\n",
    "    plt.matshow(training_x[i, :].reshape(28, 28), vmin = 0., vmax = 1., cmap = plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#solver\n",
    "import utils\n",
    "input_dims = training_x.shape[1]\n",
    "hidden_dims =[32, 32]\n",
    "output_dims =2\n",
    "    \n",
    "params = {\n",
    "    'n_epoches':30,\n",
    "    'batch_size':128,\n",
    "    'learning_rate':0.001,\n",
    "    'keep_prob': 0.6,\n",
    "    'input_dim' : input_dims,\n",
    "    'output_dim' : output_dims,\n",
    "    'hidden_dims': hidden_dims,\n",
    "    'adv_k' : 64,\n",
    "    'K' : 32,\n",
    "    'L' : 64\n",
    "}\n",
    "args = utils.ParamWrapper(params)\n",
    "\n",
    "#bais\n",
    "train_bais = np.array([[1., 1]], dtype = np.float32)\n",
    "test_bais = np.array([[1., 1]], dtype = np.float32)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "feed_forward_model = FFN_lh_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iteation is 100, with training loss 0.0462 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0531 and test accuracy is 0.98415\n",
      "The iteation is 200, with training loss 0.036 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0273 and test accuracy is 0.99394\n",
      "The iteation is 300, with training loss 0.0401 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0254 and test accuracy is 0.99487\n",
      "The iteation is 400, with training loss 0.0949 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0204 and test accuracy is 0.99534\n",
      "The iteation is 500, with training loss 0.038 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0192 and test accuracy is 0.99394\n",
      "The iteation is 600, with training loss 0.0309 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0253 and test accuracy is 0.99394\n",
      "The iteation is 700, with training loss 0.0351 and batch accuracy 0.9844\n",
      "\t the test loss is 0.0245 and test accuracy is 0.9958\n",
      "The iteation is 800, with training loss 0.0154 and batch accuracy 1.0\n",
      "\t the test loss is 0.021 and test accuracy is 0.99674\n",
      "The iteation is 900, with training loss 0.00635 and batch accuracy 1.0\n",
      "\t the test loss is 0.0178 and test accuracy is 0.99814\n",
      "The iteation is 1000, with training loss 0.00317 and batch accuracy 1.0\n",
      "\t the test loss is 0.0217 and test accuracy is 0.99534\n",
      "The iteation is 1100, with training loss 0.0313 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0198 and test accuracy is 0.99767\n",
      "The iteation is 1200, with training loss 0.0517 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0144 and test accuracy is 0.99814\n",
      "The iteation is 1300, with training loss 0.0197 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0147 and test accuracy is 0.99767\n",
      "The iteation is 1400, with training loss 0.00989 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0186 and test accuracy is 0.99674\n",
      "The iteation is 1500, with training loss 0.00348 and batch accuracy 1.0\n",
      "\t the test loss is 0.024 and test accuracy is 0.99441\n",
      "The iteation is 1600, with training loss 0.00564 and batch accuracy 1.0\n",
      "\t the test loss is 0.0174 and test accuracy is 0.99674\n",
      "The iteation is 1700, with training loss 0.00565 and batch accuracy 1.0\n",
      "\t the test loss is 0.0126 and test accuracy is 0.99767\n",
      "The iteation is 1800, with training loss 0.012 and batch accuracy 1.0\n",
      "\t the test loss is 0.0177 and test accuracy is 0.9972\n",
      "The iteation is 1900, with training loss 0.0459 and batch accuracy 0.9844\n",
      "\t the test loss is 0.012 and test accuracy is 0.9986\n",
      "The iteation is 2000, with training loss 0.00314 and batch accuracy 1.0\n",
      "\t the test loss is 0.0173 and test accuracy is 0.9972\n",
      "The iteation is 2100, with training loss 0.00336 and batch accuracy 1.0\n",
      "\t the test loss is 0.0139 and test accuracy is 0.99767\n",
      "The iteation is 2200, with training loss 0.00129 and batch accuracy 1.0\n",
      "\t the test loss is 0.0235 and test accuracy is 0.99674\n",
      "The iteation is 2300, with training loss 0.00506 and batch accuracy 1.0\n",
      "\t the test loss is 0.0139 and test accuracy is 0.99674\n",
      "The iteation is 2400, with training loss 0.0105 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0166 and test accuracy is 0.9972\n",
      "The iteation is 2500, with training loss 0.00777 and batch accuracy 1.0\n",
      "\t the test loss is 0.0215 and test accuracy is 0.9972\n",
      "The iteation is 2600, with training loss 0.0148 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0152 and test accuracy is 0.9972\n",
      "The iteation is 2700, with training loss 0.0121 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0287 and test accuracy is 0.99534\n",
      "The iteation is 2800, with training loss 0.0125 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0176 and test accuracy is 0.99674\n",
      "The iteation is 2900, with training loss 0.00722 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0154 and test accuracy is 0.9986\n",
      "The iteation is 3000, with training loss 0.0252 and batch accuracy 0.9922\n",
      "\t the test loss is 0.0164 and test accuracy is 0.9986\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "_shuffle_idx = np.arange(0, training_x.shape[0])\n",
    "np.random.shuffle(_shuffle_idx)\n",
    "with sess.as_default():\n",
    "    sess.run(init)\n",
    "    best_acc= 0.\n",
    "    iterations = 0\n",
    "    display = 100\n",
    "    n_batches = training_x.shape[0] // args.batch_size\n",
    "    for epoch in range(args.n_epoches):\n",
    "        for mini_idx in range(n_batches):\n",
    "            iterations = mini_idx + 1 + epoch * n_batches\n",
    "            start_i = mini_idx * args.batch_size \n",
    "            end_i = start_i + args.batch_size \n",
    "            if end_i > training_x.shape[0]:\n",
    "                end_i = training_x.shape[0]\n",
    "            _feed_dict = {\n",
    "                feed_forward_model.x: training_x[_shuffle_idx[start_i:end_i]],\n",
    "                feed_forward_model.y: training_y[_shuffle_idx[start_i:end_i]],\n",
    "                feed_forward_model.bias: train_bais,\n",
    "                feed_forward_model.is_training: True,\n",
    "                feed_forward_model.is_adv_training: False\n",
    "            }\n",
    "            _, loss, acc = sess.run([feed_forward_model.optimizer, feed_forward_model.loss, feed_forward_model.acc], feed_dict = _feed_dict)\n",
    "            \n",
    "            if iterations % display == 0:\n",
    "                _feed_dict = {\n",
    "                    feed_forward_model.x: test_x,\n",
    "                    feed_forward_model.y: test_y,\n",
    "                    feed_forward_model.bias: test_bais,\n",
    "                    feed_forward_model.is_training: False,\n",
    "                    feed_forward_model.is_adv_training: False\n",
    "                }\n",
    "                loss2, acc2 = sess.run([feed_forward_model.loss, feed_forward_model.acc], feed_dict = _feed_dict)\n",
    "                MSG1 = \"The iteation is {0}, with training loss {1:.3} and batch accuracy {2:.4}\"\n",
    "                MSG2 = \"\\t the test loss is {0:.3} and test accuracy is {1:.5}\"\n",
    "                print MSG1.format(iterations, loss, acc)\n",
    "                print MSG2.format(loss2, acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================Test Adversarial Examples================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load adversarial examples\n",
    "save_dir = \"./MNIST\"\n",
    "\n",
    "save_path1 = os.path.join(save_dir, 'clean.data')\n",
    "save_path2 = os.path.join(save_dir, 'fgsm_10.data')\n",
    "save_path3 = os.path.join(save_dir, 'fgsm_20.data')\n",
    "save_path4 = os.path.join(save_dir, 'fgsm_50.data')\n",
    "save_path5 = os.path.join(save_dir, 'fgsm_100.data')\n",
    "save_path_label = os.path.join(save_dir, 'clean.label')\n",
    "\n",
    "save_path1_opt = os.path.join(save_dir, 'opt_5.data')\n",
    "save_path2_opt = os.path.join(save_dir, 'opt_10.data')\n",
    "save_path3_opt = os.path.join(save_dir, 'opt_20.data')\n",
    "\n",
    "samples = utils.readdata_np(save_path1)\n",
    "labels =  utils.readdata_np(save_path_label)\n",
    "adv_examples_fgm_10 =  utils.readdata_np(save_path2)\n",
    "adv_examples_fgm_20 =  utils.readdata_np(save_path3)\n",
    "adv_examples_fgm_50 =  utils.readdata_np(save_path4)\n",
    "adv_examples_fgm_100 =  utils.readdata_np(save_path5)\n",
    "\n",
    "adv_examples_opt_5 =  utils.readdata_np(save_path1_opt)\n",
    "adv_examples_opt_10 =  utils.readdata_np(save_path2_opt)\n",
    "adv_examples_opt_20 =  utils.readdata_np(save_path3_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_adv_sampels(sess, adv_samples, labels):\n",
    "    _feed_dicts = {\n",
    "        feed_forward_model.x: adv_samples,\n",
    "        feed_forward_model.y: labels,\n",
    "        feed_forward_model.bias:test_bais,\n",
    "        feed_forward_model.is_training: False,\n",
    "        feed_forward_model.is_adv_training: False\n",
    "    }\n",
    "    return sess.run(feed_forward_model.acc, feed_dict = _feed_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on clean sampels is 1.000, vs that on adversarial exampels generated by fgm is 0.970, 0.680, 0.165, 0.000\n",
      "The accuracy on adversarial examples generated by optimization method is 0.885, 0.025, 0.050\n"
     ]
    }
   ],
   "source": [
    "acc_clean = evaluate_adv_sampels(sess, samples, labels)\n",
    "acc_fgs_10 = evaluate_adv_sampels(sess, adv_examples_fgm_10, labels)\n",
    "acc_fgs_20 = evaluate_adv_sampels(sess, adv_examples_fgm_20, labels)\n",
    "acc_fgs_50 = evaluate_adv_sampels(sess, adv_examples_fgm_50, labels)\n",
    "acc_fgs_100 = evaluate_adv_sampels(sess, adv_examples_fgm_100, labels)\n",
    "acc_opt_5 = evaluate_adv_sampels(sess, adv_examples_opt_5, labels)\n",
    "acc_opt_10 = evaluate_adv_sampels(sess, adv_examples_opt_10, labels)\n",
    "acc_opt_20 = evaluate_adv_sampels(sess, adv_examples_opt_20, labels)\n",
    "print(\"The accuracy on clean sampels is %.3f, vs that on adversarial exampels generated by fgm is %.3f, %.3f, %.3f, %.3f\" \\\n",
    "      % (acc_clean, acc_fgs_10, acc_fgs_20, acc_fgs_50, acc_fgs_100))\n",
    "print(\"The accuracy on adversarial examples generated by optimization method is %.3f, %.3f, %.3f\" % \\\n",
    "      (acc_opt_5, acc_opt_10, acc_opt_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
